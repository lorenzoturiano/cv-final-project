# Training configuration

# Training parameters
num_epochs: 50
learning_rate: 1e-3
weight_decay: 1e-2
scheduler: cosine  # Options: cosine, step, none
optimizer: adamw  # Options: adam, sgd, adamw
batch_size: 64
num_workers: 8

# Loss function parameters
loss_fn: mse # Options: mse, l1, huber

# Checkpointing
checkpoint_dir: ${results_dir}/${experiment_name}/checkpoints
save_every: 10
resume_from_checkpoint: null

# Miscellaneous
seed: 42
mixed_precision: false
